{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Clean-up notebook\n",
    "# Documentation\n",
    "\n",
    "# Topics for further research:\n",
    "# More advanced adapters from the transformers library\n",
    "# Prompt Tuning and Prefix Tuning\n",
    "# Quantization\n",
    "# RAGs, Agents and LangChain\n",
    "# Additional MLFlow features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krython/.cache/pypoetry/virtualenvs/real-estate-analyst-E5A_dmlT-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback, DataCollatorWithPadding, AdamW, get_scheduler\n",
    "from tqdm import tqdm\n",
    "from peft import get_peft_model, LoraConfig, PromptTuningConfig, PrefixTuningConfig, TaskType\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, auc\n",
    "import mlflow\n",
    "import mlflow.transformers\n",
    "import petname\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set the device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample dataset\n",
    "dataset = load_dataset(\"imdb\", split=[\"train\", \"test\"])\n",
    "train_test_split = dataset[0].train_test_split(test_size=0.1, seed=42, stratify_by_column='label')\n",
    "\n",
    "dataset = {\n",
    "    \"train\": train_test_split[\"train\"],  # train split\n",
    "    \"validation\": train_test_split[\"test\"], # validation split\n",
    "    \"test\": dataset[1],   # test split\n",
    "}\n",
    "\n",
    "dataset = DatasetDict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(sum(dataset['train']['label']) / len(dataset['train']['label']))\n",
    "print(sum(dataset['validation']['label']) / len(dataset['validation']['label']))\n",
    "print(sum(dataset['test']['label']) / len(dataset['test']['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22500\n",
      "2500\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset['train']['label']))\n",
    "print(len(dataset['validation']['label']))\n",
    "print(len(dataset['test']['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that evaluates a model on the test dataset using PyTorch\n",
    "def eval_model(model, eval_dataset, eval_batch_size, collate_function=None):\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(eval_dataset, batch_size=eval_batch_size, pin_memory=True, num_workers=4,\n",
    "                                             collate_fn=collate_function)\n",
    "    pbar = tqdm(total=len(dataloader))\n",
    "\n",
    "    if collate_function is None:\n",
    "        lable_col_name = 'label'\n",
    "    else:\n",
    "        lable_col_name = 'labels'\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['attention_mask'].to(model.device)\n",
    "        labels = batch[lable_col_name].to(model.device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            #logits = outputs.logits.to(torch.float16) - no improvement in speed\n",
    "\n",
    "            # Get predictions: the index of the max value in each row (for classification)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        # Append predictions and true labels\n",
    "        predictions.append(preds)\n",
    "        true_labels.append(labels)\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Convert lists to tensors for easy comparison\n",
    "    predictions = torch.cat(predictions)\n",
    "    true_labels = torch.cat(true_labels)\n",
    "\n",
    "    correct = (predictions == true_labels).sum().item()  # Number of correct predictions\n",
    "    total = true_labels.size(0)  # Total number of examples\n",
    "    accuracy = correct / total  # Accuracy as the fraction of correct predictions\n",
    "\n",
    "    # Calculate accuracy\n",
    "    print(f\"Model accuracy on provided set: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    # Unpack predictions and labels\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=1)\n",
    "    probs = logits[:, 1]\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(labels, predictions)  # Accuracy\n",
    "    precision = precision_score(labels, predictions, average=\"binary\")  # Precision\n",
    "    recall = recall_score(labels, predictions, average=\"binary\")  # Recall\n",
    "    f1 = f1_score(labels, predictions, average=\"binary\")  # F1 Score\n",
    "\n",
    "    # ROC-AUC\n",
    "    roc_auc = roc_auc_score(labels, probs)\n",
    "\n",
    "    # Precision-Recall AUC\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(labels, probs)\n",
    "    pr_auc = auc(recall_vals, precision_vals)\n",
    "\n",
    "    return {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1\": f1,\n",
    "    \"roc_auc\": roc_auc,\n",
    "    \"pr_auc\": pr_auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 22500/22500 [00:04<00:00, 5555.16 examples/s]\n",
      "Map: 100%|██████████| 2500/2500 [00:00<00:00, 6408.85 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:02<00:00, 10701.44 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random weights - 50% achieved accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The head is initialized with random weights so the results will be 0.5 accuracy\n",
    "eval_model(model, tokenized_dataset[\"test\"], 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification head fine-tuning - 85% achieved accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MLFlow experiment\n",
    "experiment_name = \"head_fine_tune\"\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating output folder and run name\n",
    "run_id = petname.Generate(words=2, separator='_')\n",
    "run_id += f\"_{random.randint(1000, 9999)}\"\n",
    "output_dir = f\"./results/{run_id}\"\n",
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters except for the classifier\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all trainable parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Trainable parameter: {name}\")\n",
    "    else:\n",
    "        print(f\"Frozen parameter: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments tensorboard\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=4, # 4 needed for early stopping with patience of 3\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='loss',\n",
    "    logging_first_step=True,\n",
    "    logging_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,   # Number of evaluation steps with no improvement before stopping\n",
    "    early_stopping_threshold=0.0  # Minimum improvement to reset patience\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer class \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=run_id, log_system_metrics=True):\n",
    "    # Log custom model parameters\n",
    "    mlflow.log_param(\"dataset_version\", \"imbd test\")\n",
    "    mlflow.log_param(\"model_name\", \"my_test\")\n",
    "    mlflow.log_param(\"learning_rate\", training_args.learning_rate)\n",
    "    mlflow.log_param(\"num_train_epochs\", training_args.num_train_epochs)\n",
    "    mlflow.log_param(\"batch_size\", training_args.per_device_train_batch_size)\n",
    "\n",
    "    #mlflow.transformers.autolog()\n",
    "    trainer.train()\n",
    "\n",
    "# TODO: There are a lot more features in MLflow - for example auto-logging, versioning, serving models and datasets etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should have the same results as the evaluation function\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(model, tokenized_dataset[\"test\"], 16, data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./results/head_fine_tune\")\n",
    "tokenizer.save_pretrained(\"./results/head_fine_tune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load fine-tuned model\n",
    "# fine_tuned_model = AutoModelForSequenceClassification.from_pretrained(\"./results/head_fine_tune\")\n",
    "# fine_tuned_model.to(device)\n",
    "\n",
    "# # Load the tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"./results/head_fine_tune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full fine-tuning - achieved accuracy 93%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MLFlow experiment\n",
    "experiment_name = \"custom_training_loop\"\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating output folder and run name\n",
    "run_id = petname.Generate(words=2, separator='_')\n",
    "run_id += f\"_{random.randint(1000, 9999)}\"\n",
    "output_dir = f\"./results/{run_id}\"\n",
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "train_batch_size = 16\n",
    "eval_batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "num_epochs = 3\n",
    "log_training_loss_steps = 50\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"train\"], shuffle=True, batch_size=train_batch_size, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"validation\"], batch_size=eval_batch_size, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the data prep is correct\n",
    "for batch in train_dataloader:\n",
    "    input_ids = batch['input_ids'].to(model.device)\n",
    "    attention_mask = batch['attention_mask'].to(model.device)\n",
    "    labels = batch['labels'].to(model.device)\n",
    "\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    logits = outputs.logits\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    probs = torch.nn.functional.softmax(logits) \n",
    "    correct = (preds == labels).sum().item()\n",
    "    total = labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(outputs.loss)\n",
    "    #print(logits)\n",
    "    print(preds)\n",
    "    #print(probs)\n",
    "    print(accuracy)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all trainable parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Trainable parameter: {name}\")\n",
    "    else:\n",
    "        print(f\"Frozen parameter: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimizer and learning rate\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rate scheduler\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop and validation evaluation loop\n",
    "with mlflow.start_run(run_name=run_id, log_system_metrics=True):\n",
    "    # Log custom model parameters\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "    mlflow.log_param(\"num_train_epochs\", num_epochs)\n",
    "    mlflow.log_param(\"train_batch_size\", train_batch_size)\n",
    "    mlflow.log_param(\"eval_batch_size\", eval_batch_size)\n",
    "    mlflow.log_param(\"weight_decay\", weight_decay)\n",
    "\n",
    "    model.train()\n",
    "    step = 0  # Used for logging\n",
    "\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    for epoch in range(num_epochs):\n",
    "        logits_train_list = []\n",
    "        labels_train_list = []\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            labels = batch['labels']\n",
    "            logits_train_list.append(logits.detach().cpu().numpy())\n",
    "            labels_train_list.append(labels.cpu().numpy())\n",
    "\n",
    "            if step % log_training_loss_steps == 0:\n",
    "                mlflow.log_metric(\"training_loss\", loss.item(), step=step)\n",
    "\n",
    "            step += 1\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # Calculate additional metrics for the training set\n",
    "        logits_train = np.concatenate(logits_train_list, axis=0)\n",
    "        labels_train = np.concatenate(labels_train_list, axis=0)\n",
    "        train_metrics = compute_metrics((logits_train, labels_train))\n",
    "\n",
    "        # Log train metrics to MLflow\n",
    "        for train_metric_name, train_metric_value in train_metrics.items():\n",
    "            mlflow.log_metric(f\"{train_metric_name}_train\", train_metric_value, step=epoch)\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        model.eval()\n",
    "        logits_list = []\n",
    "        labels_list = []\n",
    "        eval_loss = 0\n",
    "\n",
    "        eval_bar = tqdm(range(len(eval_dataloader)), desc=f\"Evaluating Epoch\")\n",
    "\n",
    "        for batch in eval_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            eval_loss += outputs.loss.item()\n",
    "            logits = outputs.logits\n",
    "            labels = batch['labels'] \n",
    "            logits_list.append(logits.cpu().numpy())\n",
    "            labels_list.append(labels.cpu().numpy())\n",
    "            eval_bar.update(1)\n",
    "\n",
    "        eval_bar.close()\n",
    "\n",
    "        logits = np.concatenate(logits_list, axis=0)\n",
    "        labels = np.concatenate(labels_list, axis=0)\n",
    "\n",
    "        metrics = compute_metrics((logits, labels))\n",
    "\n",
    "        # Log evaluation loss and metrics to MLflow\n",
    "        mlflow.log_metric(\"eval_loss\", eval_loss / len(eval_dataloader), step=epoch)\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value, step=epoch)\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Additional features that can be implemented:\n",
    "# bf16 precision\n",
    "# Saving checkpoints\n",
    "# Early stopping & keeping only necessary amount of checkpoints (patience + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "eval_model(model, tokenized_dataset[\"test\"], 16, data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./results/full_fine_tune_custom_loop\")\n",
    "tokenizer.save_pretrained(\"./results/full_fine_tune_custom_loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load fine-tuned model\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"./results/full_fine_tune_custom_loop\")\n",
    "# model.to(device)\n",
    "\n",
    "# # Load the tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"./results/full_fine_tune_custom_loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lora fine-tuning (90% accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MLFlow experiment\n",
    "experiment_name = \"lora_fine_tune\"\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating output folder and run name\n",
    "run_id = petname.Generate(words=2, separator='_')\n",
    "run_id += f\"_{random.randint(1000, 9999)}\"\n",
    "output_dir = f\"./results/{run_id}\"\n",
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all trainable parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Trainable parameter: {name}\")\n",
    "    else:\n",
    "        print(f\"Frozen parameter: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "\n",
    "# r - the rank of the low-rank matrix that will be trained\n",
    "# lora_alpha - A higher lora_alpha gives more weight to the LoRA updates, allowing the model to adapt more strongly to new tasks.\n",
    "# lora_dropout - Introduces regularization to prevent overfitting in the low-rank LoRA updates during training.\n",
    "# target_modules - to which layers of the pretrained LLM will the low rank matrices be added\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=8,\n",
    "    lora_alpha=4,\n",
    "    lora_dropout=0.2,\n",
    "    target_modules=[\"attention.q_lin\", \"attention.k_lin\"]\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all trainable parameters\n",
    "for name, param in lora_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Trainable parameter: {name}\")\n",
    "    else:\n",
    "        print(f\"Frozen parameter: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in lora_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=4, # 4 needed for early stopping with patience of 3\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='loss',\n",
    "    logging_first_step=True,\n",
    "    logging_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,   # Number of evaluation steps with no improvement before stopping\n",
    "    early_stopping_threshold=0.0  # Minimum improvement to reset patience\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer class \n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=run_id, log_system_metrics=True):\n",
    "    # Log custom model parameters\n",
    "    mlflow.log_param(\"learning_rate\", training_args.learning_rate)\n",
    "    mlflow.log_param(\"num_train_epochs\", training_args.num_train_epochs)\n",
    "    mlflow.log_param(\"batch_size\", training_args.per_device_train_batch_size)\n",
    "\n",
    "    #mlflow.transformers.autolog()\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should have the same results as the evaluation function\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "eval_model(lora_model, tokenized_dataset[\"test\"], 16, data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "lora_model.save_pretrained(\"./results/lora_fine_tune\")\n",
    "tokenizer.save_pretrained(\"./results/lora_fine_tune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load fine-tuned model\n",
    "# lora_model = AutoModelForSequenceClassification.from_pretrained(\"./results/lora_fine_tune\")\n",
    "# lora_model.to(device)\n",
    "\n",
    "# # Load the tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"./results/lora_fine_tune\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "real-estate-analyst-E5A_dmlT-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
